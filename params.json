{"name":"Kaggle","tagline":"This repository contains my codes and scripts submitted to various Kaggle competitions that I participated.","body":"## Welcome to my Kaggle GitHub Page\r\nI created my Kaggle account in Apr 2015 but really started participating in competitions from November 2015. My first competition was predicting 'Rossmann Store Sales' that ran through mid-December 2015. I tried basic Classification algorithm for my first submission to that competition to verify my position in Public Leader Board. Then slowly accelerated to other types of models.\r\n\r\n## Competitions Participated\r\n* Rossmann Store Sales - Sales predicting competition. Competition status: **Closed.**\r\n* Walmart Recruiting: Trip Type Classification - Classification of Customer Trip to Walmart stores. Competition status: **Closed.**\r\n* The Winton Stock Market Challenge - Predicting intra and end of day stock returns future. Competition status: On-going.\r\n* Prudential Life Insurance Assessment - Customer Risk classification on Life Insurance application. Competition status: On-going.\r\n* Telstra Network Disruptions -  Predict the severity of service disruptions on the network. Competition status: On-going.\r\n\r\n## Models used\r\nBefore participating in the above competitions, I analyzed the previous Kaggle competitions and user scripts. Predominantly, 'XGBoost' was used for single model submissions in both competitions. So I tried 'XGBoost' to start with and ensembles of XGBoost, Random Forest, Neural Network, Classification, and GBM.\r\n\r\n**Rossmann Store Sales**\r\n  1. Linear Model - [Link](https://github.com/socratesk/kaggle/blob/master/Rossmann/1-LinearModel.R)\r\n  2. XGBoost with Feature Engineering - [Link](https://github.com/socratesk/kaggle/blob/master/Rossmann/2-XGBoost-FeatureEngg.R)\r\n  3. XGBoost with Feature Engineering and Parameter tuning - [Link](https://github.com/socratesk/kaggle/blob/master/Rossmann/3-XGBoost-FeatureEngg-ParamTune.R)\r\n\r\n**Prudential Life Insurance**\r\n  1. Recursive Partition Classification Tree - [Link](https://github.com/socratesk/kaggle/blob/master/Prudential/1-Classification.R)\r\n  2. Random Forest - [Link](https://github.com/socratesk/kaggle/blob/master/Prudential/2-RandomForest.R)\r\n\r\n**Walmart Recruiting**\r\n  1. XGBoost with Feature Engineering - [Link](https://github.com/socratesk/kaggle/blob/master/Walmart-1/1-XGBoost-FeatureEngg.R)\r\n  2. Random Forest with Feature Engineering - [Link](https://github.com/socratesk/kaggle/blob/master/Walmart-1/2-RandomForest-FeatureEngg.R)\r\n\r\n**The Winton Stock Market Challenge**\r\n\r\nWorking in this competition was bit more challenging than others due to the file size and volume of data.<br>\r\n* Train data set: 40000 rows x 211 features. File size: 173 MB<br>\r\n* Test data set: 120000 rows x 147 features. File size: 282 MB\r\n\r\n\r\nWith initial model, preparation of output file was taking hours to process. So had to findout alternate ways to process them faster. To do that task, ended-up developing Java code \r\n\r\n  1. Median replacement - [R Code](https://github.com/socratesk/kaggle/blob/master/Winton/1-MedianRepl.R); [Java Code](https://github.com/socratesk/kaggle/blob/master/Winton/1_Output.java)\r\n  2. Median replacement 3% Adjusted - [R Code](https://github.com/socratesk/kaggle/blob/master/Winton/2-Adjusted_3perc_MedianRepl.R)\r\n  3. Moving average on Test data - [R Code](https://github.com/socratesk/kaggle/blob/master/Winton/3-MovingAverage.R); [Java Code](https://github.com/socratesk/kaggle/blob/master/Winton/3_Csvfile.java)\r\n\r\n## Focus\r\nKaggle's recruiting competition prohibits participants to post their code in the forum and form a group or team. This helps each individual to exhibit their own idea and expertise in Data Science area - not hijacked by others idea. This really helps me evaluate where I stand, as an individual, when compared to other participants across the World. \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}